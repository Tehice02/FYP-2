"""
ML Predictor Module
Loads trained XGBoost models and performs predictions on network flow features
"""

import numpy as np
import pickle
import json
import time
from datetime import datetime
from utils.config import Config
from utils.logger import nids_logger, log_ml_prediction, log_error, performance_logger


class MLPredictor:
    """Machine Learning prediction engine for NIDS"""
    
    def __init__(self):
        """Initialize ML predictor"""
        self.binary_model = None
        self.multiclass_model = None
        self.scaler_binary = None
        self.scaler_multiclass = None
        self.is_loaded = False
        self.prediction_count = 0
        self.attack_count = 0
        
        log_ml_prediction("ML Predictor initialized")
    
    def _set_model_to_cpu(self, model):
        """Force XGBoost model to use CPU to avoid GPU/CPU mismatch warnings"""
        try:
            # Check if it's an XGBoost model
            if hasattr(model, 'get_booster'):
                booster = model.get_booster()
                # Set device to CPU
                booster.set_param({'device': 'cpu', 'tree_method': 'hist'})
                log_ml_prediction("Model device set to CPU")
        except Exception as e:
            # If setting device fails, it's okay - model will still work
            pass
    
    def load_models(self):
        """Load trained models from disk"""
        try:
            import joblib
            
            log_ml_prediction("Loading ML models...")
            
            # Load binary classification model
            try:
                self.binary_model = joblib.load(Config.BINARY_MODEL_PATH)
                log_ml_prediction(f"Binary model loaded from {Config.BINARY_MODEL_PATH}")
                # Force CPU mode to avoid GPU/CPU mismatch warnings
                self._set_model_to_cpu(self.binary_model)
            except FileNotFoundError:
                log_error(f"Binary model not found at {Config.BINARY_MODEL_PATH}", component='MLPredictor')
                log_ml_prediction("⚠️  Binary model not found - creating dummy model for testing")
                self.binary_model = self._create_dummy_binary_model()
            
            # Load multiclass classification model
            try:
                self.multiclass_model = joblib.load(Config.MULTICLASS_MODEL_PATH)
                log_ml_prediction(f"Multiclass model loaded from {Config.MULTICLASS_MODEL_PATH}")
                # Force CPU mode to avoid GPU/CPU mismatch warnings
                self._set_model_to_cpu(self.multiclass_model)
            except FileNotFoundError:
                log_error(f"Multiclass model not found at {Config.MULTICLASS_MODEL_PATH}", component='MLPredictor')
                log_ml_prediction("⚠️  Multiclass model not found - creating dummy model for testing")
                self.multiclass_model = self._create_dummy_multiclass_model()
            
            # Load binary scaler if exists
            try:
                self.scaler_binary = joblib.load('models/model_files/scaler_binary.pkl')
                log_ml_prediction("Binary scaler loaded from models/model_files/scaler_binary.pkl")
            except FileNotFoundError:
                log_ml_prediction("No binary scaler found - will use raw features for binary model")
                self.scaler_binary = None
            
            # Load multiclass scaler if exists
            try:
                self.scaler_multiclass = joblib.load('models/model_files/scaler_multiclass.pkl')
                log_ml_prediction("Multiclass scaler loaded from models/model_files/scaler_multiclass.pkl")
            except FileNotFoundError:
                log_ml_prediction("No multiclass scaler found - will use raw features for multiclass model")
                self.scaler_multiclass = None
            
            self.is_loaded = True
            log_ml_prediction("[OK] All ML models loaded successfully")
            return True
            
        except Exception as e:
            log_error(f"Failed to load models: {str(e)}", component='MLPredictor')
            self.is_loaded = False
            return False
    
    def _create_dummy_binary_model(self):
        """Create dummy binary model for testing when real model is not available"""
        from sklearn.ensemble import RandomForestClassifier
        
        # Create and train a dummy model
        model = RandomForestClassifier(n_estimators=10, random_state=42)
        X_dummy = np.random.rand(100, Config.NUM_FEATURES)
        y_dummy = np.random.randint(0, 2, 100)
        model.fit(X_dummy, y_dummy)
        
        log_ml_prediction("Created dummy binary model for testing")
        return model
    
    def _create_dummy_multiclass_model(self):
        """Create dummy multiclass model for testing when real model is not available"""
        from sklearn.ensemble import RandomForestClassifier
        
        # Create and train a dummy model
        model = RandomForestClassifier(n_estimators=10, random_state=42)
        X_dummy = np.random.rand(100, Config.NUM_FEATURES)
        y_dummy = np.random.randint(0, 9, 100)  # 9 attack types
        model.fit(X_dummy, y_dummy)
        
        log_ml_prediction("Created dummy multiclass model for testing")
        return model
    
    def predict(self, feature_vector, flow_info=None):
        """
        Make prediction on feature vector
        
        Args:
            feature_vector: Numpy array of shape (1, 34) with network flow features
            flow_info: Dictionary with additional flow information (src_ip, dst_ip, etc.)
            
        Returns:
            Dictionary with prediction results
        """
        if not self.is_loaded:
            log_error("Models not loaded - cannot make prediction", component='MLPredictor')
            return None
        
        try:
            # Start performance timer
            performance_logger.start_timer('prediction')
            
            # Validate feature vector
            feature_vector = self._validate_features(feature_vector)
            
            # Scale features for binary model if scaler exists
            binary_features = feature_vector
            if self.scaler_binary:
                try:
                    import pandas as pd
                    # Convert to DataFrame with feature names to avoid sklearn warning
                    df = pd.DataFrame(feature_vector, columns=Config.FEATURE_NAMES)
                    binary_features = self.scaler_binary.transform(df)
                except:
                    # Fallback: suppress warning and transform as is
                    import warnings
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore")
                        binary_features = self.scaler_binary.transform(feature_vector)
            
            # Binary prediction (Attack vs Benign)
            # Ensure CPU numpy array (fix XGBoost GPU/CPU mismatch warning)
            binary_features = np.asarray(binary_features, dtype=np.float32)
            
            # Suppress XGBoost warnings (device is already set to CPU in load_models)
            import warnings
            with warnings.catch_warnings():
                warnings.filterwarnings('ignore', category=UserWarning)
                warnings.filterwarnings('ignore', message='.*tree method.*')
                warnings.filterwarnings('ignore', message='.*device.*')
                binary_pred = self.binary_model.predict(binary_features)[0]
                binary_proba = self.binary_model.predict_proba(binary_features)[0]
            
            # Prepare result
            result = {
                'timestamp': time.time(),
                'is_attack': bool(binary_pred),
                'binary_prediction': int(binary_pred),
                'binary_proba': binary_proba.tolist(),
                'confidence_score': float(binary_proba[1]) if binary_pred == 1 else float(binary_proba[0])
            }
            
            # Add flow information if provided
            if flow_info:
                result.update({
                    'src_ip': flow_info.get('src_ip', 'unknown'),
                    'dst_ip': flow_info.get('dst_ip', 'unknown'),
                    'src_port': flow_info.get('src_port', 0),
                    'dst_port': flow_info.get('dst_port', 0),
                    'protocol': flow_info.get('protocol', 'unknown')
                })
            
            # If attack detected, get attack type
            if binary_pred == 1:
                # Scale features for multiclass model if scaler exists
                multiclass_features = feature_vector
                if self.scaler_multiclass:
                    try:
                        import pandas as pd
                        # Convert to DataFrame with feature names to avoid sklearn warning
                        df = pd.DataFrame(feature_vector, columns=Config.FEATURE_NAMES)
                        multiclass_features = self.scaler_multiclass.transform(df)
                    except:
                        # Fallback: suppress warning and transform as is
                        import warnings
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")
                            multiclass_features = self.scaler_multiclass.transform(feature_vector)
                
                # Ensure CPU numpy array (fix XGBoost GPU/CPU mismatch warning)
                multiclass_features = np.asarray(multiclass_features, dtype=np.float32)
                
                # Suppress XGBoost device mismatch warnings
                import warnings
                with warnings.catch_warnings():
                    warnings.filterwarnings('ignore', category=UserWarning)
                    multiclass_pred = self.multiclass_model.predict(multiclass_features)[0]
                    multiclass_proba = self.multiclass_model.predict_proba(multiclass_features)[0]
                
                # Map prediction to attack type
                attack_type = Config.ATTACK_TYPES[multiclass_pred] if multiclass_pred < len(Config.ATTACK_TYPES) else 'Unknown'
                
                result.update({
                    'attack_type': attack_type,
                    'multiclass_prediction': int(multiclass_pred),
                    'multiclass_proba': multiclass_proba.tolist(),
                    'attack_confidence': float(max(multiclass_proba))
                })
                
                # Calculate severity
                result['severity'] = Config.get_severity(result['confidence_score'], attack_type)
                
                self.attack_count += 1
                
            else:
                result.update({
                    'attack_type': 'Benign',
                    'severity': 'LOW'
                })
            
            # Update counters
            self.prediction_count += 1
            
            # End performance timer
            inference_time = performance_logger.end_timer('prediction')
            result['inference_time_ms'] = inference_time * 1000 if inference_time else 0
            
            # Log prediction (only log attacks to avoid spam)
            if binary_pred == 1:
                log_ml_prediction(
                    f"Attack detected: {result['attack_type']} "
                    f"(confidence: {result['confidence_score']:.2%}) "
                    f"from {result.get('src_ip', 'unknown')}"
                )
            
            return result
            
        except Exception as e:
            log_error(f"Prediction failed: {str(e)}", component='MLPredictor')
            return None
    
    def _validate_features(self, feature_vector):
        """Validate and clean feature vector"""
        # Ensure correct shape
        if len(feature_vector.shape) == 1:
            feature_vector = feature_vector.reshape(1, -1)
        
        # Check feature count
        if feature_vector.shape[1] != Config.NUM_FEATURES:
            log_error(
                f"Feature mismatch: got {feature_vector.shape[1]}, expected {Config.NUM_FEATURES}",
                component='MLPredictor'
            )
            # Pad or truncate if necessary
            if feature_vector.shape[1] < Config.NUM_FEATURES:
                padding = np.zeros((1, Config.NUM_FEATURES - feature_vector.shape[1]))
                feature_vector = np.hstack([feature_vector, padding])
            else:
                feature_vector = feature_vector[:, :Config.NUM_FEATURES]
        
        # Replace NaN and Inf values
        feature_vector = np.nan_to_num(feature_vector, nan=0.0, posinf=999999.0, neginf=-999999.0)
        
        return feature_vector
    
    def predict_batch(self, feature_vectors, flow_infos=None):
        """
        Make predictions on batch of feature vectors
        
        Args:
            feature_vectors: Numpy array of shape (n, 34)
            flow_infos: List of flow information dictionaries
            
        Returns:
            List of prediction results
        """
        if not self.is_loaded:
            return []
        
        results = []
        for i, features in enumerate(feature_vectors):
            flow_info = flow_infos[i] if flow_infos and i < len(flow_infos) else None
            result = self.predict(features.reshape(1, -1), flow_info)
            if result:
                results.append(result)
        
        return results
    
    def get_statistics(self):
        """Get prediction statistics"""
        return {
            'total_predictions': self.prediction_count,
            'attack_detections': self.attack_count,
            'benign_predictions': self.prediction_count - self.attack_count,
            'attack_rate': self.attack_count / max(self.prediction_count, 1),
            'avg_inference_time': performance_logger.get_average_time('prediction')
        }
    
    def explain_prediction(self, feature_vector, feature_names=None):
        """
        Explain why model made a prediction (simplified version)
        For full SHAP implementation, you'd need the shap library
        
        Args:
            feature_vector: The feature vector that was predicted
            feature_names: List of feature names
            
        Returns:
            Dictionary with top contributing features
        """
        if feature_names is None:
            feature_names = Config.FEATURE_NAMES
        
        try:
            # Get feature importances from the model
            if hasattr(self.binary_model, 'feature_importances_'):
                importances = self.binary_model.feature_importances_
                
                # Get top 5 most important features
                top_indices = np.argsort(importances)[-5:][::-1]
                
                explanation = {
                    'top_features': [feature_names[i] for i in top_indices],
                    'feature_importances': [float(importances[i]) for i in top_indices],
                    'feature_values': [float(feature_vector[0][i]) for i in top_indices]
                }
                
                return explanation
            
            return None
            
        except Exception as e:
            log_error(f"Failed to explain prediction: {str(e)}", component='MLPredictor')
            return None


class ModelDriftDetector:
    """Detect if model performance is degrading over time"""
    
    def __init__(self, baseline_accuracy=None, threshold=0.1, window_size=1000):
        """
        Initialize drift detector
        
        Args:
            baseline_accuracy: Expected model accuracy (default from Config)
            threshold: Acceptable accuracy drop before alerting
            window_size: Number of predictions to consider for drift detection
        """
        self.baseline_accuracy = baseline_accuracy or Config.BASELINE_ACCURACY
        self.threshold = threshold
        self.window_size = window_size
        self.recent_predictions = []
        self.drift_detected = False
        
        log_ml_prediction(f"Drift detector initialized (baseline: {self.baseline_accuracy:.2%})")
    
    def check_drift(self, prediction):
        """
        Check if model is drifting
        
        Args:
            prediction: Prediction result dictionary
            
        Returns:
            Dictionary with drift status
        """
        self.recent_predictions.append({
            'confidence': prediction['confidence_score'],
            'timestamp': time.time()
        })
        
        # Keep only recent predictions
        if len(self.recent_predictions) > self.window_size:
            self.recent_predictions = self.recent_predictions[-self.window_size:]
        
        # Only check drift if we have enough predictions
        if len(self.recent_predictions) >= self.window_size:
            avg_confidence = np.mean([p['confidence'] for p in self.recent_predictions])
            
            # Check if confidence has dropped significantly
            if avg_confidence < (self.baseline_accuracy - self.threshold):
                if not self.drift_detected:
                    log_ml_prediction(
                        f"⚠️  MODEL DRIFT DETECTED: "
                        f"Average confidence ({avg_confidence:.2%}) is below baseline "
                        f"({self.baseline_accuracy:.2%})"
                    )
                    self.drift_detected = True
                
                return {
                    'drift_detected': True,
                    'current_confidence': avg_confidence,
                    'baseline': self.baseline_accuracy,
                    'drop': self.baseline_accuracy - avg_confidence,
                    'message': 'Model performance degradation detected - consider retraining'
                }
            else:
                self.drift_detected = False
        
        return {'drift_detected': False}
    
    def reset(self):
        """Reset drift detector"""
        self.recent_predictions = []
        self.drift_detected = False
        log_ml_prediction("Drift detector reset")
